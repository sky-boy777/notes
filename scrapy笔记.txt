爬虫数据去重：
存入数据库，redis
url去重：能够唯一判别一条数据的情况
根据内容去重：md5，sha1加密，存入redis，来一条数据再加密，跟数据库对比
布隆过滤器

创建
scrapy  startproject  项目名称

生成
cd  项目名称
scrapy  genspider  爬虫代码文件名（不用加.py后缀）  请求范围（如 baidu.com）

或
scrapy genspider -t crawl 爬虫代码文件名（不用加.py后缀）  请求范围（如 baidu.com）

启动
scrapy crawl 爬虫代码文件名（不用加.py后缀）

scrapy-redis分布式：
使用redis集合做去重，不重复的请求放入队列存
会将请求参数自动排列才做去重操作，如:
http://www.baidu.com/?a=1&b=2
http://www.baidu.com/?b=2&a=1
排列成：http://www.baidu.com/?a=1&b=2
然后用sha1加密请求放入去重集合

scrapy使用selenium：
在中间件将请求改为selenium，将渲染好的页面转换成response返回给爬虫

scrapy组件splash:
独立安装，一个轻量浏览器，返回JavaScript渲染完成后的网页源代码
需要安装docker

appium：
手机端自动化
-------------------------------------------------------
反爬手段：
1、基于身份识别反爬，
    headers：User-Agent字段。referer字段，从哪里过来的。通过cookie来反爬
    请求参数：html中提取。发送请求获取数据。通过js生成。通过验证码。

2、基于爬虫行为的反爬
    请求频率，记录单个ip的请求频率（ip代理池）。时间间隔（休眠）。
    请求规律（随机休眠）。设置陷阱，人只会点击页面上看得到的链接。
    设置假数据，垃圾url。阻塞网络IO，url链接大型电影

3、基于数据加密反爬
    自定义字体，css背景图片偏移，js动态生成数据，图片化数据，编码格式
---------------------------------------------------
图片验证码处理：
    使用打码平台
    手动输入
    图片识别引擎（ocr...）

爬虫总pv指页面的访问次数，每刷新一下或打开页面
爬虫：批量获取网站信息的一种方式








